{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7df36fe0",
   "metadata": {},
   "source": [
    "# A6: Imputation via Regression for Missing Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fe67ee",
   "metadata": {},
   "source": [
    "## Part A: Data Preprocessing and Imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e431c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default.payment.next.month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272.0</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>3261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331.0</td>\n",
       "      <td>14948.0</td>\n",
       "      <td>15549.0</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314.0</td>\n",
       "      <td>28959.0</td>\n",
       "      <td>29547.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940.0</td>\n",
       "      <td>19146.0</td>\n",
       "      <td>19131.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>36681.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0   1    20000.0    2          2         1   24      2      2     -1     -1   \n",
       "1   2   120000.0    2          2         2   26     -1      2      0      0   \n",
       "2   3    90000.0    2          2         2   34      0      0      0      0   \n",
       "3   4    50000.0    2          2         1   37      0      0      0      0   \n",
       "4   5    50000.0    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0  ...        0.0        0.0        0.0       0.0     689.0       0.0   \n",
       "1  ...     3272.0     3455.0     3261.0       0.0    1000.0    1000.0   \n",
       "2  ...    14331.0    14948.0    15549.0    1518.0    1500.0    1000.0   \n",
       "3  ...    28314.0    28959.0    29547.0    2000.0    2019.0    1200.0   \n",
       "4  ...    20940.0    19146.0    19131.0    2000.0   36681.0   10000.0   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default.payment.next.month  \n",
       "0       0.0       0.0       0.0                           1  \n",
       "1    1000.0       0.0    2000.0                           1  \n",
       "2    1000.0    1000.0    5000.0                           0  \n",
       "3    1100.0    1069.0    1000.0                           0  \n",
       "4    9000.0     689.0     679.0                           0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('UCI_Credit_Card.csv')\n",
    "df_original=df.copy() #Creating a copy to be used future reference\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deec4639",
   "metadata": {},
   "source": [
    "We will now introduce missing values to the columns: AGE, PAY_AMT1 and BILL_AMT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "840abc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# We will missing values to the following columns\n",
    "column1='AGE'\n",
    "column2='PAY_AMT1'\n",
    "column3='BILL_AMT1'\n",
    "\n",
    "# We now create missing values for each of this columns\n",
    "#The number of missing values is sampled randomly between 5-10% of the total number of datapoints\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "random_indices1 = random.sample(df.index.tolist(), k=int(df.shape[0]*random.uniform(0.05, 0.1)))\n",
    "random_indices2 = random.sample(df.index.tolist(), k=int(df.shape[0]*random.uniform(0.05, 0.1)))\n",
    "random_indices3 = random.sample(df.index.tolist(), k=int(df.shape[0]*random.uniform(0.05, 0.1)))\n",
    "\n",
    "df.loc[random_indices1, column1] = np.nan\n",
    "df.loc[random_indices2, column2] = np.nan\n",
    "df.loc[random_indices3, column3] = np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753322ac",
   "metadata": {},
   "source": [
    "### Simple Imputation (Baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21b8ca8",
   "metadata": {},
   "source": [
    "We will now fill the missing values of the column with the median of that column.\n",
    "\n",
    "We choose median over mean because:\n",
    "- **Extreme outliers**: Mean is sensitive to extreme outliers as the mean tends to the extreme outlier values and the generated missing values can be non-representative values. But median is unaffected by the extreme outliers.\n",
    "\n",
    "- **Skewed distribution**: When the data is skewed the mean is generally closer to the tail. But the median will give a better representation of the true center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d63b634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataset-A\n",
    "df_A=df.copy()\n",
    "\n",
    "#Replacing the columns with NaN values with their median\n",
    "pay0_median = df_A[column1].median()\n",
    "payamt1_median = df_A[column2].median()\n",
    "billamt1_median = df_A[column3].median()\n",
    "\n",
    "df_A[column1] = df_A[column1].fillna(pay0_median)\n",
    "df_A[column2] = df_A[column2].fillna(payamt1_median)\n",
    "df_A[column3] = df_A[column3].fillna(billamt1_median)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04033345",
   "metadata": {},
   "source": [
    "We will now do linear regression impuation to fill in the missing data. The critical assumption underlying regression imputation is Missing At Random (MAR).\n",
    "\n",
    "> Missing At Random (MAR) means that the probability of a value being missing is systematically related to the observed data, but is not dependent on the value of the missing data itself. \n",
    "\n",
    "The validity of this imputation hinges on this MAR assumption. Here we assume that the relationship between 'PAY_AMT1' and the other features that we observe in the complete data is the same as the relationship that would exist in the missing data if we knew the missing 'PAY_AMT1' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1e72269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "df_B=df.copy()\n",
    "df_B[column1]=df_original[column1]\n",
    "df_B[column3]=df_original[column3]\n",
    "df_missing = df_B[df_B[column2].isna()]\n",
    "df_not_missing = df_B[df_B[column2].notna()]\n",
    "\n",
    "features = [col for col in df_B.columns if col not in [column2]]\n",
    "\n",
    "X_train = df_not_missing[features]\n",
    "y_train = df_not_missing[column2]\n",
    "\n",
    "X_pred = df_missing[features]\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "predicted_values_lr = lin_reg.predict(X_pred)\n",
    "df_B.loc[df_B[column2].isna(), column2] = predicted_values_lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428d3fb3",
   "metadata": {},
   "source": [
    "To handle the non-linear relationships we will implement Non-Linear Regression Imputation using K-Nearest Neighbors Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "186ab11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "df_C=df.copy()\n",
    "df_C[column1]=df_original[column1]\n",
    "df_C[column3]=df_original[column3]\n",
    "df_missing = df_C[df_C[column2].isna()]\n",
    "df_not_missing = df_C[df_C[column2].notna()]\n",
    "\n",
    "# Define features excluding 'RESULT' and column2\n",
    "features = [col for col in df_C.columns if col not in [column2]]\n",
    "\n",
    "X_train = df_not_missing[features]\n",
    "y_train = df_not_missing[column2]\n",
    "\n",
    "X_pred = df_missing[features]\n",
    "\n",
    "# Initialize and train the KNN Regressor (you can tune n_neighbors)\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=30)\n",
    "knn_reg.fit(X_train, y_train)\n",
    "predicted_values_knn = knn_reg.predict(X_pred)\n",
    "df_C.loc[df_C[column2].isna(), column2] = predicted_values_knn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163ea140",
   "metadata": {},
   "source": [
    "## Part B: Model Training and Performance Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0078445",
   "metadata": {},
   "source": [
    "We create Dataset-D by removing all rows which have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1d25705",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_D = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a289a7ea",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c079f762",
   "metadata": {},
   "source": [
    "We now split each of the dataset into training and testing sets. We use 15% of the dataset for the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fedf6b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_A = df_A.drop('default.payment.next.month', axis=1)\n",
    "y_A = df_A['default.payment.next.month']\n",
    "X_A_train, X_A_test, y_A_train, y_A_test = train_test_split(X_A,y_A,test_size=0.15,random_state=42)\n",
    "\n",
    "X_B = df_B.drop('default.payment.next.month', axis=1)\n",
    "y_B = df_B['default.payment.next.month']\n",
    "X_B_train, X_B_test, y_B_train, y_B_test = train_test_split(X_B,y_B,test_size=0.15,random_state=42)\n",
    "\n",
    "X_C = df_C.drop('default.payment.next.month', axis=1)\n",
    "y_C = df_C['default.payment.next.month']\n",
    "X_C_train, X_C_test, y_C_train, y_C_test = train_test_split(X_C,y_C,test_size=0.15,random_state=42)\n",
    "\n",
    "X_D = df_D.drop('default.payment.next.month', axis=1)\n",
    "y_D = df_D['default.payment.next.month']\n",
    "X_D_train, X_D_test, y_D_train, y_D_test = train_test_split(X_D,y_D,test_size=0.15,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d93516",
   "metadata": {},
   "source": [
    "### Classifier Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069358ec",
   "metadata": {},
   "source": [
    "We now standardize the training data to ensure that all feature contribute equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ebb7c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_A_train)\n",
    "X_A_train_scaled = scaler.transform(X_A_train)\n",
    "\n",
    "scaler.fit(X_B_train)\n",
    "X_B_train_scaled = scaler.transform(X_B_train)\n",
    "\n",
    "scaler.fit(X_C_train)\n",
    "X_C_train_scaled = scaler.transform(X_C_train)\n",
    "\n",
    "scaler.fit(X_D_train)\n",
    "X_D_train_scaled = scaler.transform(X_D_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdf18ae",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3551070f",
   "metadata": {},
   "source": [
    "We will now train a Logistic Regression classifier on the training set of each of the four datasets- A, B, C and D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fcad57",
   "metadata": {},
   "source": [
    "#### Model-A Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8efbbd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model-A\n",
      "\n",
      "Accuracy : 0.80622\n",
      "Precision: 0.75850\n",
      "Recall   : 0.59943\n",
      "F1-Score : 0.61438\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_A_train_scaled, y_A_train.values)\n",
    "X_A_test_scaled = scaler.transform(X_A_test)\n",
    "yhat = model.predict(X_A_test_scaled)\n",
    "precision_A = precision_score(y_A_test, yhat, average='macro')\n",
    "recall_A   = recall_score(y_A_test, yhat, average='macro')\n",
    "f1_A       = f1_score(y_A_test, yhat, average='macro')\n",
    "accuracy_A= accuracy_score(y_A_test, yhat)\n",
    "print(\"\\nModel-A\\n\")\n",
    "print(f'Accuracy : {accuracy_A:.5f}')\n",
    "print(f'Precision: {precision_A:.5f}')\n",
    "print(f'Recall   : {recall_A:.5f}')\n",
    "print(f'F1-Score : {f1_A:.5f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cd6a42",
   "metadata": {},
   "source": [
    "#### Model-B Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e363528f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model-B\n",
      "\n",
      "Accuracy : 0.80556\n",
      "Precision: 0.75416\n",
      "Recall   : 0.59972\n",
      "F1-Score : 0.61473\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_B_train_scaled, y_B_train.values)\n",
    "X_B_test_scaled = scaler.transform(X_B_test)\n",
    "yhat = model.predict(X_B_test_scaled)\n",
    "precision_B = precision_score(y_B_test, yhat, average='macro')\n",
    "recall_B   = recall_score(y_B_test, yhat, average='macro')\n",
    "f1_B       = f1_score(y_B_test, yhat, average='macro')\n",
    "accuracy_B= accuracy_score(y_B_test, yhat)\n",
    "print(\"\\nModel-B\\n\")\n",
    "print(f'Accuracy : {accuracy_B:.5f}')\n",
    "print(f'Precision: {precision_B:.5f}')\n",
    "print(f'Recall   : {recall_B:.5f}')\n",
    "print(f'F1-Score : {f1_B:.5f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb10d435",
   "metadata": {},
   "source": [
    "#### Model-C Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f40ea9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model-C\n",
      "\n",
      "Accuracy : 0.80578\n",
      "Precision: 0.75472\n",
      "Recall   : 0.60022\n",
      "F1-Score : 0.61541\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_C_train_scaled, y_C_train.values)\n",
    "X_C_test_scaled = scaler.transform(X_C_test)\n",
    "yhat = model.predict(X_C_test_scaled)\n",
    "precision_C = precision_score(y_C_test, yhat, average='macro')\n",
    "recall_C   = recall_score(y_C_test, yhat, average='macro')\n",
    "f1_C       = f1_score(y_C_test, yhat, average='macro')\n",
    "accuracy_C= accuracy_score(y_C_test, yhat)\n",
    "print(\"\\nModel-C\\n\")\n",
    "print(f'Accuracy : {accuracy_C:.5f}')\n",
    "print(f'Precision: {precision_C:.5f}')\n",
    "print(f'Recall   : {recall_C:.5f}')\n",
    "print(f'F1-Score : {f1_C:.5f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51a8f25",
   "metadata": {},
   "source": [
    "#### Model-D Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6b13f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model-D\n",
      "\n",
      "Accuracy : 0.81641\n",
      "Precision: 0.76414\n",
      "Recall   : 0.61253\n",
      "F1-Score : 0.63311\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_D_train_scaled, y_D_train.values)\n",
    "X_D_test_scaled = scaler.transform(X_D_test)\n",
    "yhat = model.predict(X_D_test_scaled)\n",
    "precision_D = precision_score(y_D_test, yhat, average='macro')\n",
    "recall_D   = recall_score(y_D_test, yhat, average='macro')\n",
    "f1_D       = f1_score(y_D_test, yhat, average='macro')\n",
    "accuracy_D= accuracy_score(y_D_test, yhat)\n",
    "print(\"\\nModel-D\\n\")\n",
    "print(f'Accuracy : {accuracy_D:.5f}')\n",
    "print(f'Precision: {precision_D:.5f}')\n",
    "print(f'Recall   : {recall_D:.5f}')\n",
    "print(f'F1-Score : {f1_D:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369eca38",
   "metadata": {},
   "source": [
    "## Part C: Comparative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd42d2d",
   "metadata": {},
   "source": [
    "### Results Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66296fa2",
   "metadata": {},
   "source": [
    "We will now compare the precision, recall, f1-score and accuracy of each of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb778ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_03205 th {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_03205_row0_col0, #T_03205_row0_col1, #T_03205_row0_col2, #T_03205_row0_col3, #T_03205_row0_col4, #T_03205_row1_col0, #T_03205_row1_col1, #T_03205_row1_col2, #T_03205_row1_col3, #T_03205_row1_col4, #T_03205_row2_col0, #T_03205_row2_col1, #T_03205_row2_col2, #T_03205_row2_col3, #T_03205_row2_col4, #T_03205_row3_col0, #T_03205_row3_col1, #T_03205_row3_col2, #T_03205_row3_col3, #T_03205_row3_col4 {\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_03205\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_03205_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_03205_level0_col1\" class=\"col_heading level0 col1\" >Precision</th>\n",
       "      <th id=\"T_03205_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_03205_level0_col3\" class=\"col_heading level0 col3\" >F1-score</th>\n",
       "      <th id=\"T_03205_level0_col4\" class=\"col_heading level0 col4\" >Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_03205_row0_col0\" class=\"data row0 col0\" >Median Imputation</td>\n",
       "      <td id=\"T_03205_row0_col1\" class=\"data row0 col1\" >0.758504</td>\n",
       "      <td id=\"T_03205_row0_col2\" class=\"data row0 col2\" >0.599434</td>\n",
       "      <td id=\"T_03205_row0_col3\" class=\"data row0 col3\" >0.614376</td>\n",
       "      <td id=\"T_03205_row0_col4\" class=\"data row0 col4\" >0.806222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_03205_row1_col0\" class=\"data row1 col0\" >Linear Regression Imputation</td>\n",
       "      <td id=\"T_03205_row1_col1\" class=\"data row1 col1\" >0.754164</td>\n",
       "      <td id=\"T_03205_row1_col2\" class=\"data row1 col2\" >0.599717</td>\n",
       "      <td id=\"T_03205_row1_col3\" class=\"data row1 col3\" >0.614730</td>\n",
       "      <td id=\"T_03205_row1_col4\" class=\"data row1 col4\" >0.805556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_03205_row2_col0\" class=\"data row2 col0\" >Non-Linear Regression Imputation</td>\n",
       "      <td id=\"T_03205_row2_col1\" class=\"data row2 col1\" >0.754724</td>\n",
       "      <td id=\"T_03205_row2_col2\" class=\"data row2 col2\" >0.600215</td>\n",
       "      <td id=\"T_03205_row2_col3\" class=\"data row2 col3\" >0.615409</td>\n",
       "      <td id=\"T_03205_row2_col4\" class=\"data row2 col4\" >0.805778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_03205_row3_col0\" class=\"data row3 col0\" >Listwise Deletion</td>\n",
       "      <td id=\"T_03205_row3_col1\" class=\"data row3 col1\" >0.764140</td>\n",
       "      <td id=\"T_03205_row3_col2\" class=\"data row3 col2\" >0.612531</td>\n",
       "      <td id=\"T_03205_row3_col3\" class=\"data row3 col3\" >0.633107</td>\n",
       "      <td id=\"T_03205_row3_col4\" class=\"data row3 col4\" >0.816408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x244c4828a50>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_data = {\n",
    "    'Model':['Median Imputation','Linear Regression Imputation','Non-Linear Regression Imputation','Listwise Deletion'],\n",
    "    'Precision': [precision_A, precision_B, precision_C, precision_D],\n",
    "    'Recall': [recall_A,recall_B,recall_C,recall_D],\n",
    "    'F1-score': [f1_A,f1_B,f1_C,f1_D],\n",
    "    'Accuracy': [accuracy_A,accuracy_B,accuracy_C,accuracy_D]\n",
    "}\n",
    "summary_data = pd.DataFrame(summary_data)\n",
    "summary_output = summary_data.style.set_properties(**{'text-align': 'center'}).set_table_styles([{'selector': 'th', 'props': [('text-align', 'center')]}]).hide(axis=\"index\")\n",
    "\n",
    "# Display the styled output\n",
    "summary_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00938b98",
   "metadata": {},
   "source": [
    "### Efficacy Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841c9b47",
   "metadata": {},
   "source": [
    "<u>**Listwise Deletion (Model D)**</u>\n",
    "\n",
    "* *Advantages*\n",
    "    * In listwise deletion we remove any row which has missing values, this ensure that all the data is complete and observed (there is no artificial data)\n",
    "* *Disdvantages*\n",
    "    * There is a major reduction of sample size and because of this predictive power of the classifier trained on this dataset reduces.\n",
    "    * Listwise deletion model assumes the data is Missing Completely At Random (MCAR), i.e. the probability of a value being missing in a feature does not depend on the values of any other featture. If this is not the case the resulting dataset is no longer representative of the full population causing the final model to learn a skewed pattern and generalize poorly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb4a3c4",
   "metadata": {},
   "source": [
    "<u>**Imputation (Model A,B,C)**</u>\n",
    "\n",
    "* *Advantages*\n",
    "    * It preserves all the observations (there is no reduction in sample size), so this helps in the model to generalize better.\n",
    "* *Disdvantages*\n",
    "    * If the data is not Missing At Random (MAR), then this lead to bias in the dataset.\n",
    "    * Imputation methods like replacing missing values with median will lead to under estimation of variance thus making relations between features stronger than they actually are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4a627f",
   "metadata": {},
   "source": [
    "Listwise Deletion model perform poorly when compared with imputed models even though it does not have synthetic data beacuse:\n",
    "- When rows are deleted we might delete rows contain unique patterns, edge cases or that which belong to a minority class. This leads to major information loss.\n",
    "- A classification model requires sufficient data to learn robust decision boundaries and in Listwise Deletion model a major part of the data is deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2f527a",
   "metadata": {},
   "source": [
    "### Regression method performance (Linear vs Non-Linear) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be5a679",
   "metadata": {},
   "source": [
    "We can see that the **f1-score of non-linear model is better than that of linear model**. \n",
    "\n",
    "If there is any non linear relationship between the imputed feature and the predictors,the linear model won't be able to capture it. This is why using non-linear impuation like K-Nearest Neighbors performs better as it fills in better missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8827219",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301e0fd8",
   "metadata": {},
   "source": [
    "The best strategy is to use **Non-Linear Regression Imputation (Model C)**\n",
    "\n",
    "- When we look at the precision, recall and f1-score we see that non linear regression imputation model does the best.\n",
    "- Theoretically also it should be the best model because:\n",
    "    - Linear model will have large bias when compared to non-linear model due to its simplistic assumptionof linearity\n",
    "    - It does better than median imputation model as it captures the true covarinace while the median imputation model underestimace thecovariance\n",
    "    - It also does better than the listwise deletion model as it uses the entire dataset for training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
